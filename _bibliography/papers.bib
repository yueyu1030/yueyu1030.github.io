@inproceedings{yu2023large,
title={Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias},
author={Yu*, Yue and Zhuang*, Yuchen and Zhang*, Jieyu and Meng, Yu and Ratner, Alexander and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao},
abbr={NeurIPS (D&B Track)},
year={2023},
pdf={https://openreview.net/forum?id=6hZIfAY9GD},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
arxiv={2306.15895},
selected={true},
code={https://github.com/yueyu1030/AttrPrompt},
}

@inproceedings{zhuang2023toolqa,
title={ToolQA: A Dataset for LLM Question Answering with External Tools},
author={Zhuang*, Yuchen and Yu*, Yue and Wang*, Kuan and Sun, Haotian and Zhang, Chao},
abbr={NeurIPS (D&B Track)},
year={2023},
pdf={https://openreview.net/forum?id=pV1xV2RK6I},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
arxiv={2306.13304},
selected={true},
code={https://github.com/night-chen/ToolQA},
}

@inproceedings{bukharin2023robust,
title={Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms},
author={Alexander Bukharin and Yan Li and Yue Yu and Qingru Zhang and Zhehui Chen and Simiao Zuo and Chao Zhang and Songan Zhang and Tuo Zhao },
abbr={NeurIPS},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
pdf={https://openreview.net/forum?id=FmZVRe0gn8},
arxiv={2310.10810},
code={https://github.com/abukharin3/ERNIE}
}

@inproceedings{yu2023cold,
    title = "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
    author = "Yu, Yue  and
      Zhang, Rongzhi  and
      Xu, Ran  and
      Zhang, Jieyu  and
      Shen, Jiaming  and
      Zhang, Chao",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    abbr={ACL},
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2023.acl-long.141.pdf",
    url = "https://aclanthology.org/2023.acl-long.141",
    doi = "10.18653/v1/2023.acl-long.141",
    code = {https://github.com/yueyu1030/Patron},
    pages = "2499--2521",
    selected={true},
    abstract = "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9{\%}. Besides, with 128 labels only, PATRON achieves 91.0{\%} and 92.1{\%} of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.",
}

@inproceedings{yu2023regen,
    title = "{R}e{G}en: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval",
    author = "Yu, Yue  and
      Zhuang, Yuchen  and
      Zhang, Rongzhi  and
      Meng, Yu  and
      Shen, Jiaming  and
      Zhang, Chao",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2023.findings-acl.748.pdf",
    url = "https://aclanthology.org/2023.findings-acl.748",
    doi = "10.18653/v1/2023.findings-acl.748",
    pages = "11782--11805",
    code = {https://github.com/yueyu1030/ReGen},
    abbr={ACL Findings},
    abstract = "With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further pro- pose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self- consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that ReGen achieves 4.3{\%} gain over the strongest baselines and saves around 70{\%} of the time when compared with baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language models to boost performance.",
}

@inproceedings{zhang2023boost,
  abbr={KDD},
  title={Local Boosting for Weakly-Supervised Learning},
  author={Rongzhi Zhang and Yue Yu and Jiaming Shen and Xiquan Cui and Chao Zhang},
  booktitle={Proceedings of the 29th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 
  selected={false},
  year={2023},
  arxiv={2306.02859},
  pdf = {https://dl.acm.org/doi/pdf/10.1145/3580305.3599417},
}

@inproceedings{zhuang2023noisy,
  abbr={KDD},
  title={DyGen: Fine-Tuning Language Models with Noisy Labels by Dynamics-Enhanced Generative Modeling},
  author={Yuchen Zhuang and Yue Yu and Lingkai Kong and Xiang Chen and Chao Zhang},
  booktitle={Proceedings of the 29th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 
  selected={false},
  year={2023},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3580305.3599318},
  arxiv={2305.19395},
  code={https://github.com/night-chen/DyGen},
}

@inproceedings{kan2023rmixup,
  abbr={KDD},
  title={R-Mixup: Riemannian Mixup for Biological Networks},
  author={Xuan Kan and Zimu Li and Hejie Cui and Yue Yu and Ran Xu and Shaojun Yu and Zilong Zhang and Ying Guo and Carl Yang},
  booktitle={Proceedings of the 29th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 
  selected={false},
  year={2023},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3580305.3599483},
  arxiv={2306.02532},
}

@inproceedings{xu2023weakly,
  abbr={SIGIR},
  title={Weakly-supervised Scientific Document Classification via Retrieval-Augmented Multi-stage Training},
  author={Xu*, Ran  and Yu*, Yue and Ho, Joyce C and Yang, Carl},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval}, 
  selected={false},
  year={2023},
  arxiv={2306.07193},
  code={https://github.com/ritaranx/WANDER},
  comment={(Short Paper)}
}

@inproceedings{xu2023nest,
  abbr={AAAI},
  title={Neighborhood-regularized Self-Training for Learning with Few Labels},
  author={Xu, Ran and Yu, Yue and Cui, Hejie and Kan, Xuan and Zhu, Yanqiao and Ho, Joyce and Zhang, Chao and Yang, Carl},
  booktitle={Proceedings of the 37th AAAI Conference on Artificial Intelligence}, 
  selected={false},
  year={2023},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/26260/26032},
  code={https://github.com/ritaranx/NeST},
  arxiv={2301.03726},
  oral={true},
}

@inproceedings{yu2022causal,
  abbr={ISBI},
  title={Deep DAG Learning on Brain Networks for fMRI Analysis},
  author={Yu, Yue and Kan, Xuan and Cui, Hejie and Xu, Ran and Zheng, Yujia and Song, Xiangchen and Zhu, Yanqiao and Zhang, Kun and Nabi, Razieh and Guo, Ying and Zhang, Chao and Yang, Carl},
  booktitle={Proceedings of the 20th IEEE International Symposium on Biomedical Imaging}, 
  selected={false},
  year={2023},
  arxiv={2211.00261},
  pdf = {https://ieeexplore.ieee.org/abstract/document/10230429}
}


@inproceedings{cui2023a,
title={A Survey on Knowledge Graphs for Healthcare: Resources, Application Progress, and Promise},
author={Hejie Cui and Jiaying Lu and Shiyu Wang and Ran Xu and Wenjing Ma and Shaojun Yu and Yue Yu and Xuan Kan and Tianfan Fu and Chen Ling and Joyce Ho and Fei Wang and Carl Yang},
booktitle={ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)},
abbr={ICML (IMLH Workshop)},
year={2023},
pdf={https://openreview.net/forum?id=CZCktJoBRh},
arxiv={2306.04802}
}


@inproceedings{yu-etal-2022-coco,
    abbr={EMNLP},
    title = "{COCO}-{DR}: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning",
    author = {Yu, Yue  and
      Xiong, Chenyan  and
      Sun, Si  and
      Zhang, Chao  and
      Overwijk, Arnold},
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pdf = 	"https://aclanthology.org/2022.emnlp-main.95.pdf",
    url = "https://aclanthology.org/2022.emnlp-main.95",
    code = {https://github.com/OpenMatch/COCO-DR},
    oral={true},
    selected={false},
    pages = "1462--1479",
    abstract = "We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to improve the generalization ability of dense retrieval by combating the distribution shifts between source training tasks and target scenarios. To mitigate the impact of document differences, COCO-DR continues pretraining the language model on the target corpora to adapt the model to target distributions via COtinuous COtrastive learning. To prepare for unseen target queries, COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to reweight samples from different source query clusters for improving model robustness over rare queries during fine-tuning. COCO-DR achieves superior average performance on BEIR, the zero-shot retrieval benchmark. At BERT{\_}Base scale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At BERT{\_}Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model which has 500x more parameters. Our analysis shows the correlation between COCO-DR{'}s effectiveness in combating distribution shifts and improving zero-shot accuracy. Our code and model can be found at \url{https://github.com/OpenMatch/COCO-DR}.",
}
@inproceedings{sun-etal-2022-reduce,
    abbr={EMNLP},
    title = "Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives",
    author = "Sun, Si  and
      Xiong, Chenyan  and
      Yu, Yue  and
      Overwijk, Arnold  and
      Liu, Zhiyuan  and
      Bao, Jie",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    code = {https://github.com/OpenMatch/ANCE-Tele},
    pdf = 	"https://aclanthology.org/2022.emnlp-main.445.pdf",
    url = "https://aclanthology.org/2022.emnlp-main.445",
    pages = "6639--6654",
    abstract = "In this paper, we investigate the instability in the standard dense retrieval training, which iterates between model training and hard negative selection using the being-trained model. We show the catastrophic forgetting phenomena behind the training instability, where models learn and forget different negative groups during training iterations. We then propose ANCE-Tele, which accumulates momentum negatives from past iterations and approximates future iterations using lookahead negatives, as {``}teleportations{''} along the time axis to smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms previous state-of-the-art systems of similar size, eliminates the dependency on sparse retrieval negatives, and is competitive among systems using significantly more (50x) parameters. Our analysis demonstrates that teleportation negatives reduce catastrophic forgetting and improve convergence speed for dense retrieval training. The source code of this paper is available at https://github.com/OpenMatch/ANCE-Tele.",
}

@inproceedings{zhuang-etal-2022-resel,
    title = "{R}e{S}el: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select",
    abbr={EMNLP},
    author = "Zhuang, Yuchen  and
      Li, Yinghao  and
      Zhang, Junyang  and
      Yu, Yue  and
      Mou, Yingjun  and
      Chen, Xiang  and
      Song, Le  and
      Zhang, Chao",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pdf = 	"https://aclanthology.org/2022.emnlp-main.46.pdf",
    url = "https://aclanthology.org/2022.emnlp-main.46",
    pages = "730--744",
    oral={true},
    abstract = "We study the problem of extracting N-ary relation tuples from scientific articles. This task is challenging because the target knowledge tuples can reside in multiple parts and modalities of the document. Our proposed method ReSel decomposes this task into a two-stage procedure that first retrieves the most relevant paragraph/table and then selects the target entity from the retrieved component. For the high-level retrieval stage, ReSel designs a simple and effective feature set, which captures multi-level lexical and semantic similarities between the query and components. For the low-level selection stage, ReSel designs a cross-modal entity correlation graph along with a multi-view architecture, which models both semantic and document-structural relations between entities. Our experiments on three scientific information extraction datasets show that ReSel outperforms state-of-the-art baselines significantly.",
}

@inproceedings{yu-etal-2022-actune,
    abbr={NAACL},
    title = "{A}c{T}une: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
    author = "Yu, Yue  and
      Kong, Lingkai  and
      Zhang, Jieyu  and
      Zhang, Rongzhi  and
      Zhang, Chao",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    selected={true},
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    pdf = 	 {https://aclanthology.org/2022.naacl-main.102.pdf},
    url = "https://aclanthology.org/2022.naacl-main.102",
    doi = "10.18653/v1/2022.naacl-main.102",
    pages = "1422--1436",
    oral = {true},
    code = {https://github.com/yueyu1030/actune},
    abstract = "Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training. Additionally, we design (1) a region-aware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model{'}s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2{\%} on average. Our implementation is available at \url{https://github.com/yueyu1030/actune}.",
}

@inproceedings{zuo-etal-2022-self,
    abbr={NAACL Findings},
    title = "Self-Training with Differentiable Teacher",
    author = "Zuo*, Simiao  and
      Yu*, Yue  and
      Liang, Chen  and
      Jiang, Haoming  and
      Er, Siawpeng  and
      Zhang, Chao  and
      Zhao, Tuo  and
      Zha, Hongyuan",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    pdf = 	 {https://aclanthology.org/2022.findings-naacl.70.pdf},
    url = "https://aclanthology.org/2022.findings-naacl.70",
    doi = "10.18653/v1/2022.findings-naacl.70",
    pages = "933--949",
    abstract = "Self-training achieves enormous success in various semi-supervised and weakly-supervised learning tasks. The method can be interpreted as a teacher-student framework, where the teacher generates pseudo-labels, and the student makes predictions. The two models are updated alternatingly. However, such a straightforward alternating update rule leads to training instability. This is because a small change in the teacher may result in a significant change in the student. To address this issue, we propose DRIFT, short for differentiable self-training, that treats teacher-student as a Stackelberg game. In this game, a leader is always in a more advantageous position than a follower. In self-training, the student contributes to the prediction performance, and the teacher controls the training process by generating pseudo-labels. Therefore, we treat the student as the leader and the teacher as the follower. The leader procures its advantage by acknowledging the follower{'}s strategy, which involves differentiable pseudo-labels and differentiable sample weights. Consequently, the leader-follower interaction can be effectively captured via Stackelberg gradient, obtained by differentiating the follower{'}s strategy. Experimental results on semi- and weakly-supervised classification and named entity recognition tasks show that our model outperforms existing approaches by large margins.",
}

@inproceedings{zhang-etal-2022-prompt,
    abbr={ACL},
    title = "Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning",
    author = "Zhang, Rongzhi  and
      Yu, Yue  and
      Shetty, Pranav  and
      Song, Le  and
      Zhang, Chao",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    pdf = 	 {https://aclanthology.org/2022.acl-long.55.pdf},
    url = "https://aclanthology.org/2022.acl-long.55",
    doi = "10.18653/v1/2022.acl-long.55",
    pages = "745--758",
    abstract = "Weakly-supervised learning (WSL) has shown promising results in addressing label scarcity on many NLP tasks, but manually designing a comprehensive, high-quality labeling rule set is tedious and difficult. We study interactive weakly-supervised learning{---}the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL model. Our proposed model, named PRBoost, achieves this goal via iterative prompt-based rule discovery and model boosting. It uses boosting to identify large-error instances and discovers candidate rules from them by prompting pre-trained LMs with rule templates. The candidate rules are judged by human experts, and the accepted rules are used to generate complementary weak labels and strengthen the current model. Experiments on four tasks show PRBoost outperforms state-of-the-art WSL baselines up to 7.1{\%}, and bridges the gaps with fully supervised models.",
}


@InProceedings{pmlr-v193-xu22a,
  abbr={ML4H},
  title = 	 {Counterfactual and Factual Reasoning over Hypergraphs for Interpretable Clinical Predictions on EHR},
  author =       {Xu, Ran and Yu, Yue and Zhang, Chao and Ali, Mohammed K and Ho, Joyce C and Yang, Carl},
  booktitle = 	 {Proceedings of the 2nd Machine Learning for Health symposium},
  pages = 	 {259--278},
  year = 	 {2022},
  editor = 	 {Parziale, Antonio and Agrawal, Monica and Joshi, Shalmali and Chen, Irene Y. and Tang, Shengpu and Oala, Luis and Subbaswamy, Adarsh},
  volume = 	 {193},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28 Nov},
  publisher =    {PMLR},
  best={true},
  pdf = 	 {https://proceedings.mlr.press/v193/xu22a/xu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v193/xu22a.html},
  code = {https://github.com/ritaranx/CACHE},
  abstract = 	 {Electronic Health Record modeling is crucial for digital medicine. However, existing models ignore higher-order interactions among medical codes and their causal relations towards downstream clinical predictions. To address such limitations, we propose a novel framework CACHE, to provide <em>effective</em> and <em>insightful</em> clinical predictions based on hypergraph representation learning and counterfactual and factual reasoning techniques. Experiments on two real EHR datasets show the superior performance of CACHE. Case studies with a domain expert illustrate a preferred capability of CACHE in generating clinically meaningful interpretations towards the correct predictions.}
}



@article{zhang2022survey,
  title={A survey on programmatic weak supervision},
  author={Zhang*, Jieyu and Hsieh*, Cheng-Yu and Yu*, Yue and Zhang, Chao and Ratner, Alexander},
  journal={arXiv preprint arXiv:2202.05433},
  year={2022},
  arxiv={2202.05433}
}




@inproceedings{yu2021fine,
  title={Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach},
  author={Yu*, Yue and Zuo*, Simiao and Jiang, Haoming and Ren, Wendi and Zhao, Tuo and Zhang, Chao},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1063--1077},
  abbr={NAACL},
  selected={false},
  oral={true},
  year={2021},
  pdf={https://aclanthology.org/2021.naacl-main.84},
  arxiv={2010.07835},
  code={https://github.com/yueyu1030/COSINE}
}

@article{yu2021sumgnn,
  title={SumGNN: multi-typed drug interaction prediction via efficient knowledge graph summarization},
  author={Yu*, Yue and Huang*, Kexin and Zhang, Chao and Glass, Lucas M and Sun, Jimeng and Xiao, Cao},
  journal={Bioinformatics},
  volume={37},
  number={18},
  pages={2988--2995},
  year={2021},
  abbr={BioInf},
  publisher={Oxford University Press},
  pdf={https://academic.oup.com/bioinformatics/article/37/18/2988/6189090},
  code={https://github.com/yueyu1030/SumGNN},
  arxiv={2010.01450}
}

@inproceedings{
zhang2021wrench,
title={{WRENCH}: A Comprehensive Benchmark for Weak Supervision},
author={Jieyu Zhang and Yue Yu and Yinghao Li and Yujing Wang and Yaming Yang and Mao Yang and Alexander Ratner},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2021},
abbr={NeurIPS (D&B Track)},
oral={true},
selected={true},
url={https://openreview.net/forum?id=Q9SKS5k8io},
arxiv={2109.11377},
pdf={https://openreview.net/pdf?id=Q9SKS5k8io},
code={https://github.com/JieyuZ2/wrench},
}

@inproceedings{yu2020steam,
  title={STEAM: Self-supervised taxonomy expansion with mini-paths},
  author={Yu, Yue and Li, Yinghao and Shen, Jiaming and Feng, Hao and Sun, Jimeng and Zhang, Chao},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
  pages={1026--1035},
  abbr={KDD},
  oral={true},
  year={2020},
  arxiv={2006.10217},
  pdf={https://dl.acm.org/doi/abs/10.1145/3394486.3403145},
  code={https://github.com/yueyu1030/STEAM}
}

@inproceedings{liang2020bond,
  title={BOND: BERT-assisted open-domain named entity recognition with distant supervision},
  author={Liang*, Chen and Yu*, Yue and Jiang*, Haoming and Er, Siawpeng and Wang, Ruijia and Zhao, Tuo and Zhang, Chao},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
  pages={1054--1064},
  abbr={KDD},
  oral={true},
  year={2020},
  arxiv={2006.15509},
  code={https://github.com/cliang1453/BOND},
  pdf={https://dl.acm.org/doi/abs/10.1145/3394486.3403149}
}



@inproceedings{zhang2020seqmix,
    title = "{S}eq{M}ix: Augmenting Active Sequence Labeling via Sequence Mixup",
    author = "Zhang, Rongzhi  and
      Yu, Yue  and
      Zhang, Chao",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    preferred = "https://aclanthology.org/2020.emnlp-main.691",
    doi = "10.18653/v1/2020.emnlp-main.691",
    pages = "8566--8579",
    abbr={EMNLP},
    arxiv={2010.02322},
    code={https://github.com/rz-zhang/SeqMix},
    abstract = "Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27{\%}{--}3.75{\%} in terms of $F_1$ scores. The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.",
}

@inproceedings{yu2020semantic,
  abbr={UbiComp},
  title={Semantic-aware spatio-temporal app usage representation via graph convolutional network},
  author={Yu, Yue and Xia, Tong and Wang, Huandong and Feng, Jie and Li, Yong},
  booktitle={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  abbr={IMWUT/UbiComp},
  volume={4},
  number={3},
  pages={1--24},
  year={2020},
  publisher={ACM New York, NY, USA},
  pdf={https://dl.acm.org/doi/abs/10.1145/3411817}
}

@article{zhang2020urban,
  title={Urban anomaly analytics: Description, detection, and prediction},
  author={Zhang, Mingyang and Li, Tong and Yu, Yue and Li, Yong and Hui, Pan and Zheng, Yu},
  journal={IEEE Transactions on Big Data},
  volume={8},
  number={3},
  pages={809--826},
  year={2020},
  abbr={TBD},
  publisher={IEEE},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080109},
  arxiv={2004.12094}
}

@article{xia2020understanding,
  title={Understanding Urban Dynamics via State-Sharing Hidden Markov Model},
  author={Xia, Tong and Li, Yong and Yu, Yue and Xu, Fengli and Liao, Qingmin and Jin, Depeng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={10},
  pages={3468--3481},
  year={2020},
  abbr={TKDE},
  pdf={https://ieeexplore.ieee.org/abstract/document/8964408},
  publisher={IEEE}
}


@inproceedings{xia2019understanding,
  abbr={WWW},
  title={Understanding Urban Dynamics via State-sharing Hidden Markov Model},
  author={Xia*, Tong and Yu*, Yue and Xu, Fengli and Sun, Funing and Guo, Diansheng and Jin, Depeng and Li, Yong},
  booktitle={Proceedings of the World Wide Web Conference},
  pdf={https://dl.acm.org/doi/abs/10.1145/3308558.3313453},
  pages={3363--3369},
  year={2019}
}

@inproceedings{gao2019privacy,
  abbr={IMWUT/UbiComp},
  title={Privacy-preserving cross-domain location recommendation},
  author={Gao, Chen and Huang, Chao and Yu, Yue and Wang, Huandong and Li, Yong and Jin, Depeng},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={3},
  number={1},
  pages={1--21},
  year={2019},
  pdf={https://dl.acm.org/doi/abs/10.1145/3314398},
  publisher={ACM New York, NY, USA}
}