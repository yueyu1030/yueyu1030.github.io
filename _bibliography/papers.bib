@misc{yu2024rankrag,
    title={RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs},
    author={Yue Yu and Wei Ping and Zihan Liu and Boxin Wang and Jiaxuan You and Chao Zhang and Mohammad Shoeybi and Bryan Catanzaro},
    year={2024},
    arxiv={2407.02485},
    booktitle={arXiv preprint},
    abbr={preprint},
}

@inproceedings{
yu2024explanationaware,
title={Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning},
author={Yue Yu and Jiaming Shen and Tianqi Liu and Zhen Qin and Jing Nathan Yan and Jialu Liu and Chao Zhang and Michael Bendersky},
booktitle={The 62nd Annual Meeting of the Association for Computational Linguistics},
year={2024},
arxiv={2311.07099},
pdf={https://aclanthology.org/2024.acl-long.755.pdf},
abbr={ACL},
}

@inproceedings{
zhang2024arl2,
title={{ARL}2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling},
author={Lingxi Zhang and Yue Yu and Kuan Wang and Chao Zhang},
booktitle={The 62nd Annual Meeting of the Association for Computational Linguistics},
year={2024},
abbr={ACL},
arxiv={2402.13542},
pdf={https://aclanthology.org/2024.acl-long.203.pdf},
url={https://openreview.net/forum?id=ciguBePlw5}
}

@inproceedings{
xu2024ramehr,
title={{RAM}-{EHR}: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records},
author={Ran Xu* and Wenqi Shi* and Yue Yu and Yuchen Zhuang and Bowen Jin and May D. Wang and Joyce C. Ho and Carl Yang},
booktitle={The 62nd Annual Meeting of the Association for Computational Linguistics},
abbr={ACL},
pdf={https://aclanthology.org/2024.acl-short.68.pdf},
arxiv={2403.00815},
year={2024},
oral={true},
code={https://github.com/ritaranx/RAM-EHR},
}



@inproceedings{yu2023large,
title={Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias},
author={Yu*, Yue and Zhuang*, Yuchen and Zhang*, Jieyu and Meng, Yu and Ratner, Alexander and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao},
abbr={NeurIPS (D&B Track)},
year={2023},
pdf={https://openreview.net/forum?id=6hZIfAY9GD},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
arxiv={2306.15895},
selected={true},
code={https://github.com/yueyu1030/AttrPrompt},
}

@inproceedings{zhuang2023toolqa,
title={ToolQA: A Dataset for LLM Question Answering with External Tools},
author={Zhuang*, Yuchen and Yu*, Yue and Wang*, Kuan and Sun, Haotian and Zhang, Chao},
abbr={NeurIPS (D&B Track)},
year={2023},
pdf={https://openreview.net/forum?id=pV1xV2RK6I},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
arxiv={2306.13304},
selected={true},
code={https://github.com/night-chen/ToolQA},
}


@inproceedings{yu2023cold,
    title = "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
    author = "Yu, Yue  and
      Zhang, Rongzhi  and
      Xu, Ran  and
      Zhang, Jieyu  and
      Shen, Jiaming  and
      Zhang, Chao",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    abbr={ACL},
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2023.acl-long.141.pdf",
    url = "https://aclanthology.org/2023.acl-long.141",
    doi = "10.18653/v1/2023.acl-long.141",
    code = {https://github.com/yueyu1030/Patron},
    pages = "2499--2521",
    selected={true},
    abstract = "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9{\%}. Besides, with 128 labels only, PATRON achieves 91.0{\%} and 92.1{\%} of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.",
}

@inproceedings{yu2023regen,
    title = "{R}e{G}en: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval",
    author = "Yu, Yue  and
      Zhuang, Yuchen  and
      Zhang, Rongzhi  and
      Meng, Yu  and
      Shen, Jiaming  and
      Zhang, Chao",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2023.findings-acl.748.pdf",
    url = "https://aclanthology.org/2023.findings-acl.748",
    doi = "10.18653/v1/2023.findings-acl.748",
    pages = "11782--11805",
    code = {https://github.com/yueyu1030/ReGen},
    abbr={ACL Findings},
    abstract = "With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further pro- pose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self- consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that ReGen achieves 4.3{\%} gain over the strongest baselines and saves around 70{\%} of the time when compared with baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language models to boost performance.",
}





@inproceedings{yu-etal-2022-coco,
    abbr={EMNLP},
    title = "{COCO}-{DR}: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning",
    author = {Yu, Yue  and
      Xiong, Chenyan  and
      Sun, Si  and
      Zhang, Chao  and
      Overwijk, Arnold},
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pdf = 	"https://aclanthology.org/2022.emnlp-main.95.pdf",
    url = "https://aclanthology.org/2022.emnlp-main.95",
    code = {https://github.com/OpenMatch/COCO-DR},
    oral={true},
    selected={true},
    pages = "1462--1479",
    abstract = "We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to improve the generalization ability of dense retrieval by combating the distribution shifts between source training tasks and target scenarios. To mitigate the impact of document differences, COCO-DR continues pretraining the language model on the target corpora to adapt the model to target distributions via COtinuous COtrastive learning. To prepare for unseen target queries, COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to reweight samples from different source query clusters for improving model robustness over rare queries during fine-tuning. COCO-DR achieves superior average performance on BEIR, the zero-shot retrieval benchmark. At BERT{\_}Base scale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At BERT{\_}Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model which has 500x more parameters. Our analysis shows the correlation between COCO-DR{'}s effectiveness in combating distribution shifts and improving zero-shot accuracy. Our code and model can be found at \url{https://github.com/OpenMatch/COCO-DR}.",
}


@inproceedings{yu-etal-2022-actune,
    abbr={NAACL},
    title = "{A}c{T}une: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
    author = "Yu, Yue  and
      Kong, Lingkai  and
      Zhang, Jieyu  and
      Zhang, Rongzhi  and
      Zhang, Chao",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    selected={true},
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    pdf = 	 {https://aclanthology.org/2022.naacl-main.102.pdf},
    url = "https://aclanthology.org/2022.naacl-main.102",
    doi = "10.18653/v1/2022.naacl-main.102",
    pages = "1422--1436",
    oral = {true},
    code = {https://github.com/yueyu1030/actune},
    abstract = "Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training. Additionally, we design (1) a region-aware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model{'}s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2{\%} on average. Our implementation is available at \url{https://github.com/yueyu1030/actune}.",
}



@InProceedings{pmlr-v193-xu22a,
  abbr={ML4H},
  title = 	 {Counterfactual and Factual Reasoning over Hypergraphs for Interpretable Clinical Predictions on EHR},
  author =       {Xu, Ran and Yu, Yue and Zhang, Chao and Ali, Mohammed K and Ho, Joyce C and Yang, Carl},
  booktitle = 	 {Proceedings of the 2nd Machine Learning for Health symposium},
  pages = 	 {259--278},
  year = 	 {2022},
  editor = 	 {Parziale, Antonio and Agrawal, Monica and Joshi, Shalmali and Chen, Irene Y. and Tang, Shengpu and Oala, Luis and Subbaswamy, Adarsh},
  volume = 	 {193},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28 Nov},
  publisher =    {PMLR},
  best={true},
  pdf = 	 {https://proceedings.mlr.press/v193/xu22a/xu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v193/xu22a.html},
  code = {https://github.com/ritaranx/CACHE},
  abstract = 	 {Electronic Health Record modeling is crucial for digital medicine. However, existing models ignore higher-order interactions among medical codes and their causal relations towards downstream clinical predictions. To address such limitations, we propose a novel framework CACHE, to provide <em>effective</em> and <em>insightful</em> clinical predictions based on hypergraph representation learning and counterfactual and factual reasoning techniques. Experiments on two real EHR datasets show the superior performance of CACHE. Case studies with a domain expert illustrate a preferred capability of CACHE in generating clinically meaningful interpretations towards the correct predictions.}
}



@inproceedings{yu2021fine,
  title={Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach},
  author={Yu*, Yue and Zuo*, Simiao and Jiang, Haoming and Ren, Wendi and Zhao, Tuo and Zhang, Chao},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1063--1077},
  abbr={NAACL},
  selected={false},
  oral={true},
  year={2021},
  pdf={https://aclanthology.org/2021.naacl-main.84},
  arxiv={2010.07835},
  code={https://github.com/yueyu1030/COSINE}
}

@article{yu2021sumgnn,
  title={SumGNN: multi-typed drug interaction prediction via efficient knowledge graph summarization},
  author={Yu*, Yue and Huang*, Kexin and Zhang, Chao and Glass, Lucas M and Sun, Jimeng and Xiao, Cao},
  journal={Bioinformatics},
  volume={37},
  number={18},
  pages={2988--2995},
  year={2021},
  abbr={BioInf},
  publisher={Oxford University Press},
  pdf={https://academic.oup.com/bioinformatics/article/37/18/2988/6189090},
  code={https://github.com/yueyu1030/SumGNN},
  arxiv={2010.01450}
}

@inproceedings{
zhang2021wrench,
title={{WRENCH}: A Comprehensive Benchmark for Weak Supervision},
author={Jieyu Zhang and Yue Yu and Yinghao Li and Yujing Wang and Yaming Yang and Mao Yang and Alexander Ratner},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2021},
abbr={NeurIPS (D&B Track)},
oral={true},
selected={false},
url={https://openreview.net/forum?id=Q9SKS5k8io},
arxiv={2109.11377},
pdf={https://openreview.net/pdf?id=Q9SKS5k8io},
code={https://github.com/JieyuZ2/wrench},
}

@inproceedings{yu2020steam,
  title={STEAM: Self-supervised taxonomy expansion with mini-paths},
  author={Yu, Yue and Li, Yinghao and Shen, Jiaming and Feng, Hao and Sun, Jimeng and Zhang, Chao},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
  pages={1026--1035},
  abbr={KDD},
  oral={true},
  year={2020},
  arxiv={2006.10217},
  pdf={https://dl.acm.org/doi/abs/10.1145/3394486.3403145},
  code={https://github.com/yueyu1030/STEAM}
}

@inproceedings{liang2020bond,
  title={BOND: BERT-assisted open-domain named entity recognition with distant supervision},
  author={Liang*, Chen and Yu*, Yue and Jiang*, Haoming and Er, Siawpeng and Wang, Ruijia and Zhao, Tuo and Zhang, Chao},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
  pages={1054--1064},
  abbr={KDD},
  oral={true},
  year={2020},
  arxiv={2006.15509},
  code={https://github.com/cliang1453/BOND},
  pdf={https://dl.acm.org/doi/abs/10.1145/3394486.3403149}
}




@inproceedings{xia2019understanding,
  abbr={WWW},
  title={Understanding Urban Dynamics via State-sharing Hidden Markov Model},
  author={Xia*, Tong and Yu*, Yue and Xu, Fengli and Sun, Funing and Guo, Diansheng and Jin, Depeng and Li, Yong},
  booktitle={Proceedings of the World Wide Web Conference},
  pdf={https://dl.acm.org/doi/abs/10.1145/3308558.3313453},
  pages={3363--3369},
  year={2019}
}

@inproceedings{gao2019privacy,
  abbr={IMWUT/UbiComp},
  title={Privacy-preserving cross-domain location recommendation},
  author={Gao, Chen and Huang, Chao and Yu, Yue and Wang, Huandong and Li, Yong and Jin, Depeng},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={3},
  number={1},
  pages={1--21},
  year={2019},
  pdf={https://dl.acm.org/doi/abs/10.1145/3314398},
  publisher={ACM New York, NY, USA}
}